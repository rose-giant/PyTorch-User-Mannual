{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Model Architecture Definition Sample Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Module and Custom Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(torch.Tensor(out_features, in_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weight.T + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "\n",
    "layer = CustomLinear(in_features=3, out_features=2)\n",
    "x = torch.rand(4, 3) \n",
    "output = layer(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = nn.Tanh()\n",
    "\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "y = tanh(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "y = relu(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "y = sigmoid(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Optimization step\n",
    "loss = (output - torch.tensor([[1.0]])).pow(2).mean()\n",
    "loss.backward()\n",
    "optimizer.step()  # Update weights\n",
    "optimizer.zero_grad()  # Reset gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = FCNN(input_size=784, hidden_size=128, output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.fc = nn.Linear(64 * 7 * 7, 10)  # Assuming input size is 28x28\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # Output and hidden state\n",
    "        out = self.fc(out[:, -1, :])  # Last time-step\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(input_size=10, hidden_size=20, num_layers=2, output_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x.mean(dim=1))  # Global average pooling\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Transformer(input_dim=512, num_heads=8, num_layers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 64))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, 784), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(input_dim=16, hidden_dim=32, output_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Large and Complex Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class HybridDeepNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(HybridDeepNetwork, self).__init__()\n",
    "\n",
    "        # Convolutional Neural Network (CNN) module\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Fully Connected Neural Network (FCNN) module\n",
    "        self.fcnn = nn.Sequential(\n",
    "            nn.Linear(128 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Transformer module (BERT-based)\n",
    "        self.transformer = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # LSTM module\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5,\n",
    "        )\n",
    "\n",
    "        # Classification layer\n",
    "        self.fc = nn.Linear(256 * 2, num_classes)\n",
    "\n",
    "    def forward(self, images, text_inputs):\n",
    "        # Process images through CNN\n",
    "        x = self.cnn(images)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fcnn(x)\n",
    "        transformer_output = self.transformer(**text_inputs)\n",
    "        text_features = transformer_output.last_hidden_state[:, 0, :]  \n",
    "        combined_features = torch.cat((x, text_features), dim=1)\n",
    "        combined_features = combined_features.unsqueeze(1) \n",
    "        lstm_output, _ = self.lstm(combined_features)\n",
    "        lstm_output = lstm_output[:, -1, :]  \n",
    "        output = self.fc(lstm_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = HybridDeepNetwork(num_classes=1000)\n",
    "images = torch.randn(16, 3, 224, 224)  \n",
    "text_inputs = {\n",
    "    \"input_ids\": torch.randint(\n",
    "        0, 30522, (16, 50)\n",
    "    )\n",
    "    ,\n",
    "    \"attention_mask\": torch.ones(16, 50),\n",
    "    \"token_type_ids\": torch.zeros(16, 50),\n",
    "}\n",
    "output = model(images, text_inputs)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
